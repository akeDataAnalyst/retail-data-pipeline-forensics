{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c8cf03-6188-4995-bf53-abdacc712441",
   "metadata": {},
   "source": [
    "#### Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49516957-d5a4-46fd-8c7c-18c558ded4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully.\n",
      "Initial Row Count: 5100\n",
      "Missing Values:\n",
      "Transaction_ID       0\n",
      "Date                 0\n",
      "Product_Name         0\n",
      "Stock_On_Hand        0\n",
      "Sales_Qty            0\n",
      "Unit_Price_AED       0\n",
      "Event             1092\n",
      "Store_Location       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset generated in Phase 1\n",
    "df = pd.read_csv('raw_retail_data.csv')\n",
    "\n",
    "print(\"Data Loaded Successfully.\")\n",
    "print(f\"Initial Row Count: {len(df)}\")\n",
    "# Quick check for the irregularities we injected\n",
    "print(f\"Missing Values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4eb58-50a0-4758-9f06-a31c817ef7d1",
   "metadata": {},
   "source": [
    "#### Forensic Audit Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404a1828-c9d4-4a33-bf88-be4fb1e62082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit engine initialized.\n"
     ]
    }
   ],
   "source": [
    "audit_log = {}\n",
    "\n",
    "def log_issue(description, count):\n",
    "    audit_log[description] = count\n",
    "    print(f\"CHECK: Found {count} {description}\")\n",
    "\n",
    "print(\"Audit engine initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e29c9-b3c7-48c8-8cb1-07f7fb047bc0",
   "metadata": {},
   "source": [
    "#### Independent Identification of Discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f9e7b5-d258-48b1-b699-40bc75a9d540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK: Found 100 Duplicate Transactions\n",
      "CHECK: Found 51 Negative Stock Records\n",
      "CHECK: Found 1092 Records with Missing Event Labels\n",
      "CHECK: Found 5 Extreme Sales Outliers\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for Duplicate Transactions\n",
    "dupes = df.duplicated(subset=['Transaction_ID']).sum()\n",
    "log_issue(\"Duplicate Transactions\", dupes)\n",
    "\n",
    "# 2. Check for Negative Stock (The 'Impossible' Data)\n",
    "neg_stock = len(df[df['Stock_On_Hand'] < 0])\n",
    "log_issue(\"Negative Stock Records\", neg_stock)\n",
    "\n",
    "# 3. Check for Null Events\n",
    "null_ev = df['Event'].isnull().sum()\n",
    "log_issue(\"Records with Missing Event Labels\", null_ev)\n",
    "\n",
    "# 4. Check for Sales Outliers (Statistical Discrepancy)\n",
    "# We use the Interquartile Range (IQR) to find anomalies\n",
    "Q3 = df['Sales_Qty'].quantile(0.75)\n",
    "Q1 = df['Sales_Qty'].quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + (3 * IQR) # Strict outlier threshold\n",
    "outliers = len(df[df['Sales_Qty'] > upper_bound])\n",
    "log_issue(\"Extreme Sales Outliers\", outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fb01e-cce3-4576-b104-6a4ff50e0997",
   "metadata": {},
   "source": [
    "#### Executing the Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b1eeba-4df4-4b08-96f1-d2ea65eda955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data remediation complete. Dataset is now structurally sound.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eldu\\AppData\\Local\\Temp\\ipykernel_10920\\2200606982.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '268.75' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['Sales_Qty'] > upper_bound, 'Sales_Qty'] = upper_bound\n"
     ]
    }
   ],
   "source": [
    "# Fix Duplicates\n",
    "df = df.drop_duplicates(subset=['Transaction_ID'])\n",
    "\n",
    "# Fix Negative Stock (Assume 0 for safety)\n",
    "df.loc[df['Stock_On_Hand'] < 0, 'Stock_On_Hand'] = 0\n",
    "\n",
    "# Fix Missing Events\n",
    "df['Event'] = df['Event'].fillna('None')\n",
    "\n",
    "# Cap Outliers (To prevent skewing the KPI dashboards)\n",
    "df.loc[df['Sales_Qty'] > upper_bound, 'Sales_Qty'] = upper_bound\n",
    "\n",
    "print(\"Data remediation complete. Dataset is now structurally sound.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d100a0-4ed8-433e-b586-b9a28dfa3b21",
   "metadata": {},
   "source": [
    "#### Export Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db596708-6dbd-4b54-bac5-4af26fc061a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved as 'cleaned_retail_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('cleaned_retail_data.csv', index=False)\n",
    "print(\"Cleaned data saved as 'cleaned_retail_data.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
